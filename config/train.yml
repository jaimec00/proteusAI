# ------------------------------------------------------------------------------

# config file for training proteusAI

# ------------------------------------------------------------------------------

# Path to the YAML config file. defined here for clarity, but need to call this 
# as CL arg for it to work
config: "config/train.yml" 

# ------------------------------------------------------------------------------

# hyperparameters
hyper_parameters:

  # ----------------------------------------------------------------------------

  # main stuff
  d_model: 256
  d_latent: 1024 
  N_latent: 64
  num_aa: 20

  # ----------------------------------------------------------------------------
  
  # wf embedding params
  # no params now, everything is learnable, including wavenumbers and cb/aa scaling factors per wavenumber

  # ----------------------------------------------------------------------------

  # wf encoder params
  encoding:
    pre_process:
      d_hidden: 1024
      hidden_layers: 0
    post_process:
      d_hidden: 2048
      hidden_layers: 1
    encoders:
      self_attn:
        layers: 2
        heads: 8
        d_hidden_attn: 1024
        hidden_layers_attn: 0
      cross_attn:
        layers: 2
        heads: 16
        d_hidden_attn: 2048
        hidden_layers_attn: 0

  # ----------------------------------------------------------------------------

  # wf diffusion params
  diffusion:
    scheduler:
      alpha_bar_min: 0.00
      noise_schedule_type: "linear" 
      t_max: 100
    timestep:
      d_in: 256
      d_hidden: 1024
      hidden_layers: 0
    post_process:
      d_hidden: 1024
      hidden_layers: 0
    encoders:
      layers: 2
      heads: 4
      d_hidden_attn: 1024
      hidden_layers_attn: 0

  # ----------------------------------------------------------------------------

  # wf decoder params
  decoding:
    pre_process:
      d_hidden: 1024
      hidden_layers: 0
    post_process:
      d_hidden: 1024
      hidden_layers: 0
    encoders:
      self_attn:
        layers: 2
        heads: 8
        d_hidden_attn: 1024
        hidden_layers_attn: 0
      cross_attn:
        layers: 2
        heads: 16
        d_hidden_attn: 1024
        hidden_layers_attn: 0

  # ----------------------------------------------------------------------------

  # wf extraction params
  extraction:
    pre_process:
      d_hidden: 1024
      hidden_layers: 0
    post_process:
      d_hidden: 1024
      hidden_layers: 0
    encoders:
      layers: 4
      heads: 8
      d_hidden_attn: 1024
      hidden_layers_attn: 0

# ------------------------------------------------------------------------------

# training params
training_parameters:

  train_type: "extraction_finetune"  # options are:
                            #   extraction:
                            #     train embedding and extraction together, defines the wf space. everything else is frozen
                            #   vae:
                            #     train encoder and decoder on embedding outputs. defines the latent space. embedding, extraction, and diffusion are frozen
                            #   extraction_finetune:
                            #     train extraction on reconstructed wf (decoder output). allows extractor to learn to predict seq from noisy wf. only extractor is not frozen
                            #   diffusion:
                            #     train diffusion on encoder output (latent wf). only diffusion is not frozen
                            #   old:
                            #     old vanilla model, consists of embedder and extractor only, but embedder only encodes struct info, no aa specific scaling factors

  epochs: 100  # number of epochs

  # for pre-training
  weights:
    use_model: ""
    use_embedding_weights: "/scratch/hjc2538/projects/proteusAI/models/extraction_5_w_normed_wf/model_parameters_embedding.pth"
    use_encoding_weights: "/scratch/hjc2538/projects/proteusAI/models/VAE_protein_latent4_1emin6beta_sqrtsched_wtanh_1024dl/tmp/model_parameters_encoding.pth"
    use_diffusion_weights: ""
    use_decoding_weights: "/scratch/hjc2538/projects/proteusAI/models/VAE_protein_latent4_1emin6beta_sqrtsched_wtanh_1024dl/tmp/model_parameters_decoding.pth"
    use_extraction_weights: "/scratch/hjc2538/projects/proteusAI/models/extraction_5_w_normed_wf/model_parameters_extraction.pth"
  inference:
    temperature: !!float 1e-6
    cycles: 10
  
  # convergence criteria
  early_stopping:
    thresh: 0.00 # delta validation sequence similarity, if below this value, training is stopped. negative values mean the seq sim must decrease before stopping
    tolerance: 15 # how many epochs to consider when calculating delta seq sim for early stopping. takes the max delta seq sim between current epoch and last n epochs, and decides based on this
  
  adam:
    beta1: 0.9  # for adam optim 
    beta2: 0.98  # for adam optim
    epsilon: !!float 10e-9  # for adam optim (!!float lets PyYAML know this is a float, not str)
  
  regularization:
    dropout: 0.00  # percentage of dropout
    wf_dropout: 0.00 # dropout for wf embedding, drops out this pct of sources, diff sources dropped for diff wavenumbers
    noise_coords_std: 0.00 # stdev of noise injection into coordinates during training
    use_chain_mask: True # whether to mask all chains except the sequence cluster representative in loss computation, all chains still used for fwd pass as context though

  loss:
    # i think grad accum is v important for diffusion, will see if this helps
    accumulation_steps: 4  # grad accumulation; how many batches to process before learning step
    cel:
      label_smoothing: 0.0
    kl:
      annealing: False # only beta used if this is false
      beta: !!float 1e-6 # for beta-VAE, this serves as the max beta in kl annealing
      kappa: 0.001 # controls the steepness of the transition for kl annealing
      midpoint: 8000 # number of steps to pass the midpoint in sigmoidal kl annealing
    nll:
      gamma: 0.0 # for diffusion NLL auxiliary loss, 0 means no nll, just used for logging
    grad_clip_norm: 1.0 # max L2 norm of gradients for gradient clipping. if set to 0, no gradient clipping is applied (not recommended since the loss is a sum)
  
  lr:
    lr_type: "attn"
    lr_step: !!float 1e-3 # max lr, ramps up to this val before decreasing  
    warmup_steps: 6000 # number of warmup steps before decreasing
  
# ------------------------------------------------------------------------------

# dataset configs
data:

  data_path: "/scratch/hjc2538/projects/proteusAI/pdb_2021aug02_filtered"  # path to data for uncw hpc

  num_train: -1  # number of training samples to use; -1 means all available
  num_val: -1  # number of validation samples to use; -1 means all available
  num_test: -1  # number of test samples to use; -1 means all available

  # want as few batch_size/seq_length combinations as possible to speed up triton, since it 
  # recompiles the attention kernel for each shape
  # DataHolder class sorts samples by sequence length and splits into max_batch_size batches one time to reduce masking (avoid small seqs being grouped with large seqs), 
  # then recursively randomizes order of samples within batch, splits into two, and continues until batches are <= batch_tokens
  max_batch_size: 256  # maximum samples per batch
  min_seq_size: 64 # minimum sequence length, shorter is padded to this length
  max_seq_size: 8192 # max sequence lengths, longer samples not included
  batch_tokens: 8192 # number of valid tokens per batch (including non-representative chains, basically the tokens used in computation, not necessarily in loss)
  max_resolution: 3.5 # max_resolution of PDBs

# ------------------------------------------------------------------------------

# Output
output:
  out_path: "/scratch/hjc2538/projects/proteusAI/models/extraction_finetune_protlatent"  # path to store output, such as plots and weights file. for uncw hpc
  cel_plot: "cel_vs_epoch.png"  # path to save plot of loss vs epochs after training
  mse_plot: "mse_vs_epoch.png"  # path to save plot of loss vs epochs after training
  kldiv_plot: "kldiv_vs_epoch.png"  # path to save plot of loss vs epochs after training
  vae_plot: "vae_plot.png"
  seq_plot: "seq_sim_vs_epoch.png"  # path to save plot of sequence similarity vs epochs after training
  test_plot: "test_seq_sims.png"  # path to save plot of sequence similarity vs epochs after training, not implemented yet
  weights_path: "model_parameters.pth"  # path to save weights after training
  model_checkpoints: 10 # number of epochs to save a checkpoint of the model after

# ------------------------------------------------------------------------------

debug:
  # prints each layers gradients after each step so can check for exploding/vanishing gradients
  # note that it is printed before grad clipping
  debug_grad: False # something is wrong with vae and i have no idea what, see what autograd anomoly detector finds

# ------------------------------------------------------------------------------
