# ------------------------------------------------------------------------------

# config file for training proteusAI

# ------------------------------------------------------------------------------

# Path to the YAML config file. defined here for clarity, but need to call this 
# as CL arg for it to work
config: "config/train.yml" 

# ------------------------------------------------------------------------------

# hyperparameters
hyper_parameters:

  # ----------------------------------------------------------------------------

  # main stuff
  d_wf: 128 # d_wf // 2 is the number of wavefunctions defined for each protein, since each wf produces real and imag part
  d_v: 128
  d_e: 128
  topk: 30

  # ----------------------------------------------------------------------------
  
  node_embedding:
    min_wl: 3.5
    max_wl: 12
    base_wl: 20.0
    anisotropic: True
    learn_wl: True # learn the wavenumbers
    learn_aa: False # learn the scaling factors

  edge_embedding: # pmpnn does linearly spaced centers, not spreads, will test autoregressive first before changing this
    min_rbf: 2.0
    max_rbf: 22.0
    num_rbfs: 16.0

  encoder:
    layers: 3

  decoder:
    layers: 3

# ------------------------------------------------------------------------------

# training params
training_parameters:

  rng: 0 # for data loading

  single_chain: False # single chain or multichain model, automatically chooses the correct dataset from this, as long as defined below
  ca_only_model: False # Ca only or full backbone model, haven't tested full backbone yet

  epochs: 1000  # number of epochs, training until convergence

  checkpoint:
    path: ""
    
  inference:
    temperature: !!float 1e-6
    cycles: 10
  
  # convergence criteria
  early_stopping:
    thresh: 0.00 # delta validation sequence similarity, if below this value, training is stopped. negative values mean the seq sim must decrease before stopping
    tolerance: 30 # how many epochs to consider when calculating delta seq sim for early stopping. takes the max delta seq sim between current epoch and last n epochs, and decides based on this
  
  adam:
    beta1: 0.90  # decay rate of momentum term
    beta2: 0.98  # decay rate of variance term
    epsilon: !!float 10e-9  # for numerical stability in param updates (!!float lets PyYAML know this is a float, not str)
    weight_decay: 0.00 # weight decay, set to 0 for no weightdecay, ie regular adam

  regularization:
    dropout: 0.10  # percentage of dropout
    noise_coords_std: 0.00 # stdev of noise injection into coordinates during training
    use_chain_mask: True # whether to mask all chains except the sequence cluster representative in loss computation, all chains still used for fwd pass as context though
    homo_thresh: 0.70
    label_smoothing: 0.00

  loss:
    # i think grad accum is v important for diffusion, will see if this helps
    accumulation_steps: 1  # grad accumulation; how many batches to process before learning step. i think mlm needs better grad approx, will see
                            # note that this depends on if doing multi gpu training, if have two gpus with accum=1, then each step occurs after 1 batch per gpu --> 2 batches in this case
    token_based_step: False # false means that accum refers to number of batches processed, true means it refers to the number of valid tokens processed, where valid means the loss is computed for it
    grad_clip_norm: 0.0 # max L2 norm of gradients for gradient clipping. if set to 0, no gradient clipping is applied (not recommended since the loss is a sum)
  
  lr:
    lr_type: "attn"
    lr_step: !!float 1e-3 # max lr, ramps up to this val before decreasing  
    warmup_steps: 4000 # number of warmup steps before decreasing

# ------------------------------------------------------------------------------

# dataset configs
data:

  multi_chain_data_path: "/scratch/hjc2538/projects/proteusAI/data/multi_chain/processed"  # path to data for multi-chain model (dataset from Dapauras et. al.)
  single_chain_data_path: "/scratch/hjc2538/projects/proteusAI/data/single_chain/processed"  # path to data for single chain model (dataset from Ingraham et. al.)

  num_train: -1  # number of training samples to use; -1 means all available
  num_val: -1  # number of validation samples to use; -1 means all available
  num_test: -1  # number of test samples to use; -1 means all available

  # want as few batch_size/seq_length combinations as possible to speed up triton, since it 
  # recompiles the attention kernel for each shape
  # DataHolder class sorts samples by sequence length and splits into max_batch_size batches one time to reduce masking (avoid small seqs being grouped with large seqs), 
  # then recursively randomizes order of samples within batch, splits into two, and continues until batches are <= batch_tokens
  max_batch_size: 256  # maximum samples per batch
  min_seq_size: 16 # minimum sequence length, shorter is padded to this length, this is necessary due to triton matmul, min of 16, but with the option of having larger min
  max_seq_size: 16384 # max sequence lengths, longer samples not included
  batch_tokens: 16384 # number of valid tokens per batch (including non-representative chains, basically the tokens used in computation, not necessarily in loss)
  max_resolution: 3.5 # max_resolution of PDBs

# ------------------------------------------------------------------------------

# Output
output:
  out_path: "/scratch/hjc2538/projects/proteusAI/models/gnn/no_decoder"
  model_checkpoints: 10 # number of epochs to save a checkpoint of the model after

# ------------------------------------------------------------------------------

debug:
  # prints each layers gradients after each step so can check for exploding/vanishing gradients
  # note that it is printed before grad clipping
  debug_grad: False # something is wrong with vae and i have no idea what, see what autograd anomoly detector finds
  print_losses: False
  print_grad_L2: False

port: 29500 # in case want to run multiple independant instances of the training on a single machine (one for each gpu), you need different ports for each

# ------------------------------------------------------------------------------

