# ------------------------------------------------------------------------------

# config file for training proteusAI

# ------------------------------------------------------------------------------

# Path to the YAML config file. defined here for clarity, but need to call this as CL arg for it to work
config: "config/train.yml" 

# hyperparameters
hyper_parameters:

  d_model: 512 # dimensionality of input embeddings

  wf:
    learnable_wavelengths: False # whether to make wavelengths learnable, comparable performance either way, but seems better without
    wf_mag_type: 1  # 0 mean no magnitude scaling (all sources are fully felt by all observers regardless of distance)
                    # 1 means scale like in greens func (1/|R|)
                    # 2 means do log scaling (1/log2(|R|)) so that distant sources have a bigger effect than in regular greens func
                      # note that layer norm is applied after, so the base you use for type 2 does not matter, 
                      # as that is equivilant to multiplying all interactions by the same constant (log2(base))
                    # 3 means do 1/sqrt(|R|) so that distant sources have bigger effect, but not as aggressive as log scaling.
    anisotropic_wf: True # whether to use anisotropic version that uses Cb info. finally ready (: AND FUCKING WORKING !!!!!!! will prob remove this option since it is better overall than isotropic
    min_wl: 2.0  # minimum wl to use in wavelength sampling
    max_wl: 10.0  # maximum wl to use in wavelength sampling
    base_wl: 25.0  # minimum base to use in wavelength sampling
    d_hidden_we: 2048 # hidden layer dim in WF embedding MLP
    hidden_layers_we: 0 # number of hidden layers,
    use_aa: False # whether to use aa context, only applicable if anisotropic is true. if use_aa is false, all tokens are the mask token 

  # note that changing some of these parameters (and others, like d_model) requires triton to recompile the geometric attention kernel, 
  # so first few iterations may be slow, but will speed up after most of the input shapes have been seen at least once (see dataset configuration section for more details)
  # the parameters that cause recompilation have two pound signs ##, instead of one # next to them
  struct_encoders:

    layers: 8 # number of structure encoder layers
    num_heads: 8  ## number of attention heads to perform the training with

    learnable_spreads: True # whether to make the spreads learnable, new version shows small gains when making spreads learnable (set to False if beta is 0.0, see below)
    
    # if you want all heads to have the same spread, set min_spread=max_spread. in this case the base used does not matter. otherwise, base must be > 1
    min_spread: 3 # min spread RBFs in attention
    max_spread: 15 # max spread
    base_spread: 1.0 # base to use for logarithmic sampling, set to 1 for linear sampling
    num_spread: 32 # only matters if spreads are learnable, if not this is automatically set to num_heads
    
    # if you want global attention, set min_rbf to 0.0 and max_rbf to 1.0, 0.001 and 0.999 w/ rbf scaling (beta approx. 1) seems to work better.
    # note, that the max rbf term as 0.999 only excludes distances of 0.0 (unless you choose a huge max spread), so it basically just stops the model from allowing tokens to attend to themselves 
    # it does worse w/ maxRBF=1, but i think this is because ive only tested with beta>=1, so the attention to itself is overemphasized since rbf term is always 1. will try maxrbf=1 and beta<1 to test if this helps
    min_rbf: 0.001 ## for masking spatially distant residue pairs in attention computation
    max_rbf: 0.850 ## for masking spatially close residue pairs, ensures RBF bias doesnt skew the attention logits so that mean is still approx. 0
    # maxrbf of 0.9 is working, i think because this excludes close pairs that other heads are also processing, i.e. removes redundancy 

    # if you want no scaling (use raw attention logits), set beta to 0.0, note that masking is still done according to min and max rbf args (above)
    # note that scaling (beta>0) seems to improve performance
    beta: 2.0 ## how much to scale the RBF term by, larger values emphasize geometry, lower emphasize raw attention
    
    d_hidden_attn: 2048  # hidden dimensions of encoder feed forward layer
    hidden_layers_attn: 0 # hidden layers for ffn

# ------------------------------------------------------------------------------

# training params
training_parameters:

  epochs: 100  # number of epochs
  use_amp: False  # whether to use automatic mixed precision. 
                  # do NOT set this to true, as get exploding 
                  # gradients since my custom kernels cast certain tensor
                  # to fp32, will fix later

  # for transfer learning
  weights:
    # use_model: "/scratch/hjc2538/projects/proteusAI/models/aniso_w_aa_inject_nomask/model_parameters.pth"
    use_model: ""

  inference: # uses mask predict algo for inference. only used if hyper_parameters.wf.use_aa is true
    temperature: !!float 1e-6  # temperature for autoregressive inference (for testing). 
    num_iters: 10 # number of iterations for mask predict inference
    remask: False # whether to remask each iteration, or run with the full prediction
  
  # convergence criteria
  early_stopping:
    thresh: 0.00 # delta validation sequence similarity, if below this value, training is stopped. negative values mean the seq sim must decrease before stopping
    tolerance: 3 # how many epochs to consider when calculating delta seq sim for early stopping. takes the max delta seq sim between current epoch and last n epochs, and decides based on this
  
  adam:
    beta1: 0.9  # for adam optim 
    beta2: 0.98  # for adam optim
    epsilon: !!float 10e-9  # for adam optim (!!float lets PyYAML know this is a float, not str)
  
  regularization:
    dropout: 0.1  # percentage of dropout
    wf_dropout: 0.00 # dropout for wf embedding, drops out this pct of sources, diff sources dropped for diff wavenumbers
    attn_dropout: 0.00  # percentage of dropout for atttention, 0.0 works best, probably bc attention is already sparse
    label_smoothing: 0.0  # percentage of label smoothing to use for loss calculation
    noise_coords_std: 0.0 # stdev of noise injection into coordinates during training
    use_chain_mask: True # whether to mask all chains except the sequence cluster representative in loss computation, all chains still used for fwd pass as context though
    mask_id: 10.0 # whether mask token is the mean of all other tokens, or 0. mean seems to work much better, as it still gives the model information about orientation
    aa_scale: 10.0 # what the max aa size is, all aas scaled relative to this. 0.0 means use the raw aa sizes computed from avg volumes of aas when in the protein core

    # MASK injection
    mask_injection:
      mask_type: "noise" # whether to mask random tokens ("rand") or do span masking ("span") or no masking, just injection of noise via replacing true tokens with random tokens ("noise")
      min_mask_pct: 0.00 # pct masked is clampled to this val
      max_mask_pct: 1.00 # also clamped
      mean_span: 10 # mean span length # only applicable if mask type is span
      std_span: 5 # std span length
      randAA_pct: 0.0 # what percentage of non-masked tokens to replace with random token. -1 means each sample gets a randAA_pct sampled from uniform distribution from 0 to 1
  
  loss:
    accumulation_steps: 32  # grad accumulation; how many batches to process before learning step
    loss_type: "sum"  # whether to use the 'sum' or the 'mean' for NLL, sum works much better
    grad_clip_norm: 5.0 # max L2 norm of gradients for gradient clipping. if set to 0, no gradient clipping is applied (not recommended if loss_type is sum)
  
  lr:
    lr_step: !!float 1e-3 # max lr, ramps up to this val before decreasing  
    warmup_steps: 4000 # number of warmup steps before decreasing
  
# ------------------------------------------------------------------------------

# dataset configs
data:

  data_path: "/scratch/hjc2538/projects/proteusAI/pdb_2021aug02_filtered"  # path to data for uncw hpc

  num_train: -1  # number of training samples to use; -1 means all available
  num_val: -1  # number of validation samples to use; -1 means all available
  num_test: -1  # number of test samples to use; -1 means all available

  # want as few batch_size/seq_length combinations as possible to speed up triton, since it 
  # recompiles the geometric attention kernel for each shape
  # DataHolder class sorts samples by sequence length and splits into max_batch_size batches one time to reduce masking (avoid small seqs being grouped with large seqs), 
  # then recursively randomizes order of samples within batch, splits into two, and continues until batches are <= batch_tokens
  max_batch_size: 256  # maximum samples per batch
  min_seq_size: 64 # minimum sequence length, shorter is padded to this length
  max_seq_size: 8192 # max sequence lengths, longer samples not included
  batch_tokens: 8192 # number of valid tokens per batch
  max_resolution: 3.5 # max_resolution of PDBs

# ------------------------------------------------------------------------------

# Output
output:
  out_path: "/scratch/hjc2538/projects/proteusAI/models/8enc_8h_32accum"  # path to store output, such as plots and weights file. for uncw hpc
  loss_plot: "loss_vs_epoch.png"  # path to save plot of loss vs epochs after training
  seq_plot: "seq_sim_vs_epoch.png"  # path to save plot of sequence similarity vs epochs after training
  weights_path: "model_parameters.pth"  # path to save weights after training
  model_checkpoints: 10 # number of epochs to save a checkpoint of the model after

# ------------------------------------------------------------------------------

debug:
  # prints each layers gradients after each step so can check for exploding/vanishing gradients
  # note that it is printed before grad clipping
  debug_grad: False

# ------------------------------------------------------------------------------
