# ------------------------------------------------------------------------------

# config file for training proteusAI

# ------------------------------------------------------------------------------

# Path to the YAML config file. defined here for clarity, but need to call this as CL arg for it to work
config: "config/train.yml" 

# hyperparameters
hyper_parameters:

  d_model: 512 # dimensionality of input embeddings

  wf:
    learnable_wavelengths: False # whether to make wavelengths learnable, comparable performance either way, but seems better without
    wf_mag_type: 1  # 0 mean no magnitude scaling (all sources are fully felt by all observers regardless of distance)
                # 1 means scale like in greens func (1/|R|)
                # 2 means do log scaling (1/log2(|R|)) so that distant sources have a bigger effect than in regular greens func
                  # note that layer norm is applied after, so the base you use for type 2 does not matter, 
                  # as that is equivilant to multiplying all interactions by the same constant (log2(base))
                # 3 means do 1/sqrt(|R|) so that distant sources have bigger effect, but not as aggressive as log scaling.
    anisotropic_wf: True # whether to use anisotropic version that uses Cb info. finally ready (: AND FUCKING WORKING !!!!!!!
    min_wl: 3.7  # minimum wl to use in wavelength sampling
    max_wl: 20.0  # maximum wl to use in wavelength sampling
    base_wl: 20.0  # minimum base to use in wavelength sampling
    d_hidden_we: 1024 # hidden layer dim in WF embedding MLP
    hidden_layers_we: 0 # number of hidden layers,

  aa:
    # aa context not used, first optimizing wf embedding by itself
    use_aa: True
    d_hidden_aa: 1024 # hidden dim of mlp
    hidden_layers_aa: 3 # hidden layers
    learnable_esm: True # whether to make ESM2 embeddings learnable, only applicable if use_aa is true and esm2_weights_path is not empty 
    esm2_weights_path: "utils/model_utils/esm2/esm2_t33_650M_UR50D.pt"  # can be a path or the name of the esm2 model. 
                                                                        # see utils/model_utils/esm2/get_esm_weights.py for a 
                                                                        # description of the different weights
                                                                        # if the name is provided, fetched from facebook website
                                                                        # if not provided (or empty string), AA embeddings are learned from scratch

  # note that changing some of these parameters (and others, like d_model) requires triton to recompile the geometric attention kernel, 
  # so first few iterations may be slow, but will speed up after most of the input shapes have been seen at least once (see dataset configuration section for more details)
  # the parameters that cause recompilation have two pound signs ##, instead of one #
  struct_encoders:

    layers: 4 # number of structure encoder layers
    num_heads: 8  ## number of attention heads to perform the training with

    learnable_spreads: True # whether to make the spreads learnable, new version shows small gains when making spreads learnable (set to False if beta is 0.0, see below)
    
    # if you want all heads to have the same spread, set min_spread=max_spread. in this case the base used does not matter. otherwise, base must be > 1
    min_spread: 3 # min spread RBFs in attention
    max_spread: 20 # max spread
    base_spread: 5.0 # base to use for logarithmic sampling
    num_spread: 32 # only matters if spreads are learnable, if not this is automatically set to num_heads
    
    # if you want global attention, set min_rbf to 0.0 and max_rbf to 1.0, 0.001 and 0.999 w/ rbf scaling (beta approx. 1) seems to work better.
    # note, that the min rbf terms only excludes distances of 0.0 (unless you choose a huge max spread), so it basically just stops the model from allowing tokens to attend to themselves 
    min_rbf: 0.001 ## for masking spatially distant residue pairs in attention computation
    max_rbf: 0.999 ## for masking spatially close residue pairs, ensures RBF bias doesnt skew the attention logits so that mean is still approx. 0
    
    # if you want no scaling (use raw attention logits), set beta to 0.0, note that masking is still done according to min and max rbf args (above)
    # note that scaling (beta>0) seems to improve performance
    beta: 2.0 ## how much to scale the RBF term by, larger values emphasize geometry, lower emphasize raw attention
    
    d_hidden_attn: 1024  # intermediate dimensions of encoder feed forward layer
    hidden_layers_attn: 0 # hidden layers for ffn

  seq_encoders: # layers is the only thing used for now, just copying struct encoder configs while testing
    layers: 2 # number of sequence encoder layers

    num_heads: 8  ## number of attention heads to perform the training with
    learnable_spreads: False # whether to make the spreads learnable, new version shows small gains when making spreads learnable (set to False if beta is 0.0, see below)
    
    # if you want all heads to have the same spread, set min_spread=max_spread. in this case the base used does not matter. otherwise, base must be > 1
    min_spread: 3 # min spread RBFs in attention
    max_spread: 20 # max spread
    base_spread: 5.0 # base to use for logarithmic sampling
    num_spread: 32 # only matters if spreads are learnable, if not this is automatically set to num_heads
    
    # if you want global attention, set min_rbf to 0.0 and max_rbf to 1.0, 0.001 and 0.999 w/ rbf scaling (beta approx. 1) seems to work better
    min_rbf: 0.001 ## for masking spatially distant residue pairs in attention computation
    max_rbf: 0.999 ## for masking spatially close residue pairs, ensures RBF bias doesnt skew the attention logits so that mean is still approx. 0
    
    # if you want no scaling (use raw attention logits), set beta to 0.0, note that masking is still done according to min and max rbf args (above)
    # note that scaling (beta>0) seems to improve performance
    beta: 2.0 ## how much to scale the RBF term by, larger values emphasize geometry, lower emphasize raw attention
    
    d_hidden_attn: 2048  # intermediate dimensions of encoder feed forward layer
    hidden_layers_attn: 0 # hidden layers for ffn

  decoders: # only layers is used, everything else is copied from struct encoder
    
    layers: 2 # number of decoder layers

    num_heads: 8  ## number of attention heads to perform the training with
    learnable_spreads: False # whether to make the spreads learnable, new version shows small gains when making spreads learnable (set to False if beta is 0.0, see below)
    
    # if you want all heads to have the same spread, set min_spread=max_spread. in this case the base used does not matter. otherwise, base must be > 1
    min_spread: 3 # min spread RBFs in attention
    max_spread: 20 # max spread
    base_spread: 5.0 # base to use for logarithmic sampling
    num_spread: 32 # only matters if spreads are learnable, if not this is automatically set to num_heads
    
    # if you want global attention, set min_rbf to 0.0 and max_rbf to 1.0, 0.001 and 0.999 w/ rbf scaling (beta approx. 1) seems to work better
    min_rbf: 0.001 ## for masking spatially distant residue pairs in attention computation
    max_rbf: 0.999 ## for masking spatially close residue pairs, ensures RBF bias doesnt skew the attention logits so that mean is still approx. 0
    
    # if you want no scaling (use raw attention logits), set beta to 0.0, note that masking is still done according to min and max rbf args (above)
    # note that scaling (beta>0) seems to improve performance
    beta: 2.0 ## how much to scale the RBF term by, larger values emphasize geometry, lower emphasize raw attention
    
    d_hidden_attn: 2048  # intermediate dimensions of encoder feed forward layer
    hidden_layers_attn: 0 # hidden layers for ffn

# ------------------------------------------------------------------------------

# dataset configs
data:

  # data_path: "/share/wangyy/hjc2538/proteusAI/pdb_2021aug02_filtered"  # path to data for ncs hpc
  data_path: "/scratch/hjc2538/projects/proteusAI/pdb_2021aug02_filtered"  # path to data for uncw hpc

  num_train: -1  # number of training samples to use; -1 means all available
  num_val: -1  # number of validation samples to use; -1 means all available
  num_test: -1  # number of test samples to use; -1 means all available

  # want as few batch_size/seq_length combinations as possible to speed up triton, since it 
  # recompiles the geometric attention kernel for each shape
  # DataHolder class sorts samples by sequence length and splits into max_batch_size batches one time to reduce masking (avoid small seqs being grouped with large seqs), 
  # then recursively randomizes order of samples within batch, splits into two, and continues until batches are <= batch_tokens
  max_batch_size: 256  # maximum samples per batch
  min_seq_size: 64 # minimum sequence length, shorter is padded to this length
  max_seq_size: 8192 # max sequence lengths, longer samples not included
  batch_tokens: 8192 # number of valid tokens per batch
  max_resolution: 3.5 # max_resolution of PDBs

# ------------------------------------------------------------------------------

# training params
training_parameters:

  epochs: 100  # number of epochs
  use_amp: False  # whether to use automatic mixed precision. 
                  # do NOT set this to true, as get exploding 
                  # gradients since my custom kernels cast certain tensor
                  # to fp32, will fix later

  # for transfer learning
  weights:
    # use_model: "/scratch/hjc2538/projects/proteusAI/models/struct_only_accum8_learnspreads/model_parameters.pth" # start from pretrained model, expects path to a pt file. if empty string, learned from scratch
    use_model: ""
    freeze_structure_weights: False
    freeze_sequence_weights: False
    freeze_decoder_weights: False
    cp_struct_enc_2_seq_enc: False

  inference:
    temperature: 0.1  # temperature for autoregressive inference (for testing). only used if use_aa is true
    num_iters: 10 # number of iterations for CMLM inference, not used yet
  
  # convergence criteria
  early_stopping:
    thresh: 0.00 # delta validation sequence similarity, if below this value, training is stopped. negative values mean the seq sim must decrease before stopping
    tolerance: 3 # how many epochs to consider when calculating delta seq sim for early stopping. takes the max delta seq sim between current epoch and last n epochs, and decides based on this
  
  adam:
    beta1: 0.9  # for adam optim 
    beta2: 0.98  # for adam optim
    epsilon: !!float 10e-9  # for adam optim (!!float lets PyYAML know this is a float, not str)
  
  regularization:
    dropout: 0.1  # percentage of dropout
    wf_dropout: 0.0 # dropout for wf embedding, drops out this pct of sources, diff sources dropped for diff wavenumbers
    attn_dropout: 0.00  # percentage of dropout for atttention, 0.0 works best, probably bc attention is already sparse
    label_smoothing: 0.0  # percentage of label smoothing to use for loss calculation
    noise_coords_std: 0.0 # stdev of noise injection into coordinates during training
    use_chain_mask: True # whether to mask all chains except the sequence cluster representative in loss computation, all chains still used for fwd pass though
  
    # MASK injection
    mask_injection:
      mean_mask_pct: 0.5 # mean percentage of masked tokens per sample
      std_mask_pct: 0.25 # std percentage of masked tokens per sample
      min_mask_pct: 0.05 # pct masked is clampled to this val
      max_mask_pct: 1.00 # also clamped
      mean_span: 10 # mean span length
      std_span: 5 # std span length
      randAA_pct: 0.1 # what percentage of masked tokens to replace with a random token, helps w/ self correction in inference
      trueAA_pct: 0.1 # what percentage of masked tokens to replace with the true label
  
  loss:
    accumulation_steps: 8  # grad accumulation; how many batches to process before learning step
    loss_type: "sum"  # whether to use the 'sum' or the 'mean' for NLL, sum works much better
    grad_clip_norm: 1.0 # max L2 norm of gradients for gradient clipping. if set to 0, no gradient clipping is applied (not recommended if loss_type is sum)
  
  lr:
    lr_step: !!float 1e-3 # max lr, ramps up to this val before decreasing  
    warmup_steps: 4000 # number of warmup steps before decreasing
  
# ------------------------------------------------------------------------------

debug:
  # prints each layers gradients after each step so can check for exploding/vanishing gradients
  # note it is printed before grad clipping
  debug_grad: False

# ------------------------------------------------------------------------------

# Output
output:
  # out_path: "/share/wangyy/hjc2538/proteusAI/models/test"  # path to store output, such as plots and weights file. for nc state hpc
  out_path: "/scratch/hjc2538/projects/proteusAI/models/aniso_w_aa_inject_full_loss"  # path to store output, such as plots and weights file. for uncw hpc
  loss_plot: "loss_vs_epoch.png"  # path to save plot of loss vs epochs after training
  seq_plot: "seq_sim_vs_epoch.png"  # path to save plot of sequence similarity vs epochs after training
  weights_path: "model_parameters.pth"  # path to save weights after training
  model_checkpoints: 10 # number of epochs to save a checkpoint of the model after

# ------------------------------------------------------------------------------