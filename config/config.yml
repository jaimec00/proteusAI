# Configuration for training the model
d_model: 512  # dimensionality of input embeddings
min_wl: 1.0  # minimum wl to use in wavelength sampling
max_wl: 50.0  # maximum wl to use in wavelength sampling
min_base: 2.0  # minimum base to use in wavelength sampling
max_base: 80.0  # base to use in wavelength sampling
min_rbf: 0.05  # minimum rbf scaling factor in gaussian mha
max_rbf: 0.99  # maximum rbf scaling factor in gaussian mha
min_spread: 1
max_spread: 30
num_heads: 8  # number of attention heads to perform the training with
decoder_layers: 8 # number of decoder layers
hidden_linear_dim: 1024  # intermediate dimensions of decoder feed forward layer
temperature: 0.1  # temperature for autoregressive inference (for testing)
max_tokens: 10000  # maximum number of tokens
include_ncaa: False # whether to include non-canonical amino acids in loss calculation
use_amp: True  # whether to use automatic mixed precision
use_checkpoint: False # whether to use gradient checkpointing for the MHA module
use_chain_mask: True # whether to mask all chains except the sequence cluster representative

# Training configuration
num_train: -1  # number of training samples to use; -1 means all available
num_val: -1  # number of validation samples to use; -1 means all available
num_test: -1  # number of test samples to use; -1 means all available
epochs: 20  # number of epochs

# want as few combinations as possible to speed up triton autotuning
batch_sizes: [1, 2, 4, 8, 16, 32, 64, 128]  # possible samples per batch
seq_sizes: [128, 256, 512, 1024, 2048, 4096, 8192, 10000] # possible sequence lengths
batch_size: 20000 # number of valid tokens per batch 
autotune_wf: False # whether to perform triton autotuning on protein_to_wavefunc
autotune_mha: False # whether to perform triton autotuning on attn

accumulation_steps: 1  # grad accumulation; how many batches to process before learning step
learning_step: 0.00005  # learning rate
beta1: 0.9  # learning rate
beta2: 0.98  # learning rate
epsilon: 10e-9  # learning rate
dropout: 0.1  # percentage of dropout
label_smoothing: 0.1  # percentage of label smoothing to use on the output labels for loss calculation
loss_type: "sum"  # whether to use the 'sum' or the 'mean' for CEL
loss_sum_norm: 2000
lr_scale: 0.1  # LR scaling factor
lr_patience: 2  # LR patience for scaling down after plateau

# early_stopping: 
# model_checkpoints:

# Training type
training_type: "wf" # choices: wf, onehot, probs, or self_supervised
# if wf, phase 1 is progressive decoder expansion (if specified), phase 2 is normal learning
# if onehot, phase 1 has decoder weights frozes, and phase 2 is normal learning (everything unfrozen)
# if probs, phase 1 also has decoder weights frozen (allow context to adapt to current weights), phase 2 is normal
# if self-supervised, phase 1 has progressively increasing amounts of self-supervision, phase 2 is full self-supervision
phase_split: 0.5  # ratio of progressive learning phase
expand_decoders: False
precomputed_features: False

# ------------------------------------------------------------------------------
# these only apply to probs/self_supervised training type

# Input label smoothing 
initial_min_lbl_smooth_mean: 0.15  # initial minimum input label smoothing
final_min_lbl_smooth_mean: 0.85  # final minimum input label smoothing
max_lbl_smooth_mean: 1.05  # maximum input label smoothing
min_lbl_smooth_stdev: 0.05  # minimum input label smoothing stdev
max_lbl_smooth_stdev: 0.2  # maximum input label smoothing stdev

# Input noise (
min_noise_stdev: 0.1  # minimum mean of noise to apply to inputs
initial_max_noise_stdev: 0.2  # initial maximum mean of noise to apply to inputs
final_max_noise_stdev: 0.1  # final maximum mean of noise to apply to inputs

# Input label smoothing and noise cycle length
lbl_smooth_noise_cycle_length: 10.3  # cycle length of label smoothing and noise oscillations

# Input one-hot injection
min_one_hot_injection_mean: 0.15  # minimum mean percentage of one-hot label injection in training
initial_max_one_hot_injection_mean: 0.25  # initial maximum mean percentage of one-hot label injection in training
final_max_one_hot_injection_mean: 0.5  # final maximum mean percentage of one-hot label injection in training
one_hot_injection_stdev: 0.25  # stdev percentage of one-hot label injection in training
one_hot_injection_cycle_length: 7.4  # input one-hot injection cycle length

# ------------------------------------------------------------------------------

# Output
out_path: "/share/wangyy/hjc2538/proteusAI/models/test2"  # path to store output, such as plots and weights file
loss_plot: "loss_vs_epoch.png"  # path to save plot of loss vs epochs after training
seq_plot: "seq_sim_vs_epoch.png"  # path to save plot of sequence similarity vs epochs after training
weights_path: "model_parameters.pth"  # path to save weights after training
write_dot: True  # whether to save the dot file of the computational graph

# Input
data_path: "/share/wangyy/hjc2538/proteusAI/pdb_2021aug02_filtered"  # path to data
# use_model: ""
config: "config/config.yml"  # Path to the YAML config file
