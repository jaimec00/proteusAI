# ------------------------------------------------------------------------------

# config file for training proteusAI

# ------------------------------------------------------------------------------

# hyperparameters

d_model: 512 # dimensionality of input embeddings
learnable_wavelengths: False # whether to make wavelengths learnable, comparable performance either way, but seems better without
wf_type: 1  # 0 mean no magnitude scaling (all sources are fully felt by all observers regardless of distance)
            # 1 means scale like in greens func (1/|R|)
            # 2 means do log scaling (1/log2(|R|)) so that distant sources have a bigger effect than in regular greens func
              # note that layer norm is applied after, so the base you use for type 2 does not matter, 
              # as that is equivilant to multiplying all interactions by the same constant (log2(base))
            # 3 means do 1/sqrt(|R|) so that distant sources have bigger effect, but not as aggressive as log scaling. not implemented yet
anisotropic_wf: False # whether to use anisotropic version that uses Cb info. raises NotImplementedError if True for now
min_wl: 3.7  # minimum wl to use in wavelength sampling
max_wl: 20.0  # maximum wl to use in wavelength sampling
base_wl: 20.0  # minimum base to use in wavelength sampling
d_hidden_we: 1024 # hidden layer dim in WF embedding MLP
hidden_layers_we: 0 # number of hidden layers

# aa context not used, first optimizing wf embedding by itself
use_aa: True
d_hidden_aa: 1024 # hidden dim of mlp
hidden_layers_aa: 3 # hidden layers
learnable_esm: True # whether to make ESM2 embeddings learnable, only applicable if use_aa is true and esm2_weights_path is not empty 
esm2_weights_path: "utils/model_utils/esm2/esm2_t33_650M_UR50D.pt"  # can be a path or the name of the esm2 model. 
                                                                    # see utils/model_utils/esm2/get_esm_weights.py for a 
                                                                    # description of the different weights
                                                                    # if the name is provided, fetched from facebook website
                                                                    # if not provided (or empty string), AA embeddings are learned from scratch

# note that changing some of these parameters requires triton to recompile the geometric attention kernel, 
# so first few iterations may be slow, but will speed up after most of the input shapes have been seen at least once (see dataset configuration section for more details)

encoder_layers: 3 # number of encoder layers
num_heads: 8  # number of attention heads to perform the training with
learnable_spreads: False # whether to make the spreads learnable, new version shows small gains when making spreads learnable (set to False if beta is 0.0, see below)

# if you want all heads to have the same spread, set min_spread=max_spread. in this case the base used does not matter. otherwise, base must be > 1
min_spread: 3 # min spread RBFs in attention
max_spread: 15 # max spread
base_spread: 10.0 # base to use for logarithmic sampling
num_spread: 32 # only matters if spreads are learnable, if not this is automatically set to num_heads

# if you want global attention, set min_rbf to 0.0 and max_rbf to 1.0, 0.001 and 0.999 w/ rbf scaling (beta approx. 1) seems to work better
min_rbf: 0.001 # for masking spatially distant residue pairs in attention computation
max_rbf: 0.999 # for masking spatially close residue pairs, ensures RBF bias doesnt skew the attention logits so that mean is still approx. 0

# if you want no scaling (use raw attention logits), set beta to 0.0, note that masking is still done according to min and max rbf args (above)
# note that scaling (beta>0) seems to improve performance
beta: 1.0 # how much to scale the RBF term by, larger values emphasize geometry, lower emphasize raw attention

d_hidden_attn: 1024  # intermediate dimensions of encoder feed forward layer
hidden_layers_attn: 0 # hidden layers for ffn

temperature: 0.1  # temperature for autoregressive inference (for testing). only used if use_aa is true

# ------------------------------------------------------------------------------

# dataset configs

num_train: -1  # number of training samples to use; -1 means all available
num_val: -1  # number of validation samples to use; -1 means all available
num_test: -1  # number of test samples to use; -1 means all available
epochs: 10  # number of epochs

# want as few batch_size/seq_length combinations as possible to speed up triton, since it 
# recompiles the geometric attention kernel for each shape
# DataHolder class sorts samples by sequence length and splits into max_batch_size batches one time to reduce masking (avoid small seqs being grouped with large seqs), 
# then recursively randomizes order of samples within batch, splits into two, and continues until batches are <= batch_tokens
max_batch_size: 256  # maximum samples per batch
min_seq_size: 64 # minimum sequence length, shorter is padded to this length
max_seq_size: 8192 # max sequence lengths, longer samples not included
batch_tokens: 8192 # number of valid tokens per batch
min_resolution: 3.5 # actually max_resolution, will fix the name later

# ------------------------------------------------------------------------------

# training params

accumulation_steps: 1  # grad accumulation; how many batches to process before learning step
beta1: 0.9  # for adam optim 
beta2: 0.98  # for adam optim
epsilon: 10e-9  # for adam optim
dropout: 0.0  # percentage of dropout
attn_dropout: 0.00  # percentage of dropout for atttention
label_smoothing: 0.0  # percentage of label smoothing to use for loss calculation
loss_type: "sum"  # whether to use the 'sum' or the 'mean' for NLL
grad_clip_norm: 1.0 # max L2 norm of gradients for gradient clipping. if set to 0, no gradient clipping is applied (not recommended if loss_type is sum)

lr_type: "attn" # learning rate scheduler, "attn" is the lr scheduler used in the original attention paper, other options are plateu and cyclic

warmup_steps: 4000 # for scheduler from attn is all you need

lr_step: 1e-4  # learning rate (starting point for plateu, or max lr for attn, not used for cyclic)
lr_scale: 0.1  # LR scaling factor when plateu
lr_patience: 3  # LR patience for scaling down after plateau

# learning rate if doing cyclic
lr_initial_min: 5e-5 
lr_initial_max: 1e-4
lr_final_min: 1e-5
lr_final_max: 5e-5
lr_cycle_length: 7.0

use_chain_mask: True # whether to mask all chains except the sequence cluster representative in loss computation, all chains still used for fwd pass though
use_amp: False  # whether to use automatic mixed precision. 
                # do NOT set this to true, as get exploding 
                # gradients since my custom kernels cast certain tensor
                # to fp32, will fix later

# ------------------------------------------------------------------------------

# data augmentation
noise_coords_std: 0.10 # stdev of noise injection into coordinates during training

# Input MASK injection
initial_min_MASK_injection_mean: 1.0  # initial minimum mean percentage of MASK injection injection in training
final_min_MASK_injection_mean: 0.15  # final minimum mean percentage of MASK injection injection in training
initial_max_MASK_injection_mean: 1.0  # initial maximum mean percentage of MASK injection injection in training
final_max_MASK_injection_mean: 0.15  # final maximum mean percentage of MASK injection injection in training
MASK_injection_stdev: 0.00  # stdev percentage of MASK injection injection in training
MASK_injection_cycle_length: 20  # input MASK token injection cycle length

# ------------------------------------------------------------------------------

# prints each layers gradients after each step so can check for exploding/vanishing gradients
# note it is printed before grad clipping
debug_grad: False 

# ------------------------------------------------------------------------------

# Output
# out_path: "/share/wangyy/hjc2538/proteusAI/models/test"  # path to store output, such as plots and weights file. for nc state hpc
out_path: "/scratch/hjc2538/projects/proteusAI/models/enc_dec"  # path to store output, such as plots and weights file. for uncw hpc
loss_plot: "loss_vs_epoch.png"  # path to save plot of loss vs epochs after training
seq_plot: "seq_sim_vs_epoch.png"  # path to save plot of sequence similarity vs epochs after training
weights_path: "model_parameters.pth"  # path to save weights after training
model_checkpoints: 10 # number of epochs to save a checkpoint of the model after

# ------------------------------------------------------------------------------

# Input
# data_path: "/share/wangyy/hjc2538/proteusAI/pdb_2021aug02_filtered"  # path to data for ncs hpc
data_path: "/scratch/hjc2538/projects/proteusAI/pdb_2021aug02_filtered"  # path to data for uncw hpc
config: "config/config.yml"  # Path to the YAML config file. defined here for clarity, but need to call this as CL arg for it to work
# use_model: "/scratch/hjc2538/projects/proteusAI/models/no_aa_attn_lr/model_parameters_e69_s22.82.pth" # start from pretrained model, expects path to a pt file

# ------------------------------------------------------------------------------

# not implemented yet
# early_stopping: # delta delta seq sim threshold: diff in delta seq sim between train and val across adjacent epochs 

# ------------------------------------------------------------------------------
