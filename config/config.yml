# Configuration for training the model
d_model: 512  # dimensionality of input embeddings
min_wl: 3.7  # minimum wl to use in wavelength sampling
max_wl: 20.0  # maximum wl to use in wavelength sampling
base_wl: 20.0  # minimum base to use in wavelength sampling
d_hidden_we: 1024
hidden_layers_we: 0

d_hidden_aa: 1024
hidden_layers_aa: 0

encoder_layers: 4 # number of dualcoder layers
num_heads: 4  # number of attention heads to perform the training with
min_spread: 3.0
max_spread: 6.0
base_spread: 10.0
min_rbf: 0.01  # minimum rbf scaling factor in gaussian mha
max_rbf: 0.99  # maximum rbf scaling factor in gaussian mha
d_hidden_attn: 1024  # intermediate dimensions of decoder feed forward layer
hidden_layers_attn: 0

temperature: 0.1  # temperature for autoregressive inference (for testing)

# Training configuration
num_train: 256  # number of training samples to use; -1 means all available
num_val: -1  # number of validation samples to use; -1 means all available
num_test: 128  # number of test samples to use; -1 means all available
epochs: 3  # number of epochs

# want as few combinations as possible to speed up triton autotuning
max_batch_size: 128  # possible samples per batch
min_seq_sizes: 512 # possible sequence lengths
max_seq_sizes: 16384 # possible sequence lengths
batch_tokens: 16384 # number of valid tokens per batch 

accumulation_steps: 4  # grad accumulation; how many batches to process before learning step
learning_step: 0.0001  # learning rate
beta1: 0.9  # learning rate
beta2: 0.98  # learning rate
epsilon: 10e-9  # learning rate
dropout: 0.1  # percentage of dropout
label_smoothing: 0.1  # percentage of label smoothing to use on the output labels for loss calculation
loss_type: "mean"  # whether to use the 'sum' or the 'mean' for CEL
loss_sum_norm: 2000
lr_type: plateu
lr_initial_min: 5e-5
lr_initial_max: 1e-4
lr_final_min: 1e-5
lr_final_max: 5e-5
lr_cycle_length: 7.0

lr_scale: 0.1  # LR scaling factor
lr_patience: 5  # LR patience for scaling down after plateau
use_chain_mask: True # whether to mask all chains except the sequence cluster representative
use_amp: True  # whether to use automatic mixed precision

# ------------------------------------------------------------------------------

# Input MASK injection
min_MASK_injection_mean: 0.00  # minimum mean percentage of one-hot label injection in training
initial_max_MASK_injection_mean: 0.15  # initial maximum mean percentage of one-hot label injection in training
final_max_MASK_injection_mean: 0.05  # final maximum mean percentage of one-hot label injection in training
MASK_injection_stdev: 0.05  # stdev percentage of one-hot label injection in training
MASK_injection_cycle_length: 7.4  # input one-hot injection cycle length

# ------------------------------------------------------------------------------

# Output
out_path: "/share/wangyy/hjc2538/proteusAI/models/test"  # path to store output, such as plots and weights file
loss_plot: "loss_vs_epoch.png"  # path to save plot of loss vs epochs after training
seq_plot: "seq_sim_vs_epoch.png"  # path to save plot of sequence similarity vs epochs after training
weights_path: "model_parameters.pth"  # path to save weights after training
write_dot: True  # whether to save the dot file of the computational graph

# Input
data_path: "/share/wangyy/hjc2538/proteusAI/pdb_2021aug02"  # path to data
# use_model: 
# early_stopping: 
# model_checkpoints:
config: "config/config.yml"  # Path to the YAML config file
